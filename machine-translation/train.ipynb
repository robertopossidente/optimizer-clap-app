{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(batch_size=64, dtype='float32', epochs=10, lr=0.01, momentum=0.9, no_cuda=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loading dataset...\n",
      "Loading dataset...\n",
      "Length of train_data_loader: 14\n",
      "Length of val_data_loader: 14\n",
      "Length of test_data_loader: 14\n",
      "Use beam_size=4, alpha=0.60, K=5\n",
      "Use learning_rate=2.00\n",
      "hparams.epochs= 3\n",
      "Epoch 0 - train_one_epoch started\n",
      "[MO833] Rank,0,Initialization Time: 0.000351\n",
      "[MO833] Rank,0,Epoch,0,Iteration,0,It. time,0.618373,Elapsed time,0.667248\n",
      "[MO833] Rank,0,Epoch,0,Iteration,1,It. time,0.686359,Elapsed time,1.359832\n",
      "[MO833] Rank,0,Epoch,0,Iteration,2,It. time,1.464815,Elapsed time,2.839077\n",
      "[MO833] Rank,0,Epoch,0,Iteration,3,It. time,0.825252,Elapsed time,3.676836\n",
      "[MO833] Rank,0,Epoch,0,Iteration,4,It. time,2.313725,Elapsed time,5.995269\n",
      "[MO833] Rank,0,Epoch,0,Iteration,5,It. time,1.686809,Elapsed time,7.694652\n",
      "[MO833] Rank,0,Epoch,0,Iteration,6,It. time,1.131211,Elapsed time,8.829993\n",
      "[MO833] Rank,0,Epoch,0,Iteration,7,It. time,2.523005,Elapsed time,11.364309\n",
      "[MO833] Rank,0,Epoch,0,Iteration,8,It. time,2.011447,Elapsed time,13.383045\n",
      "[MO833] Rank,0,Epoch,0,Iteration,9,It. time,2.462182,Elapsed time,15.856173\n",
      "[MO833] Rank,0,Epoch,0,Iteration,10,It. time,1.280546,Elapsed time,17.140666\n",
      "[MO833] Rank,0,Epoch,0,Iteration,11,It. time,1.526606,Elapsed time,18.677795\n",
      "[MO833] Rank,0,Epoch,0,Iteration,12,It. time,3.195267,Elapsed time,21.877481\n",
      "[MO833] Rank,0,Epoch,0,Iteration,13,It. time,0.624407,Elapsed time,22.506414\n",
      "[MO833] Rank,0,Epoch,0,Epoch time,0.625469,Elapsed time,22.507476\n",
      "Epoch 0 - train_one_epoch finished\n",
      "Epoch 0 - after waitall\n",
      "Epoch 1 - train_one_epoch started\n",
      "[MO833] Rank,0,Initialization Time: 28.789359\n",
      "[MO833] Rank,0,Epoch,1,Iteration,0,It. time,0.111718,Elapsed time,28.913555\n",
      "[MO833] Rank,0,Epoch,1,Iteration,1,It. time,0.858986,Elapsed time,29.775333\n",
      "[MO833] Rank,0,Epoch,1,Iteration,2,It. time,0.897530,Elapsed time,30.677167\n",
      "[MO833] Rank,0,Epoch,1,Iteration,3,It. time,1.708462,Elapsed time,32.390172\n",
      "[MO833] Rank,0,Epoch,1,Iteration,4,It. time,1.682580,Elapsed time,34.083144\n",
      "[MO833] Rank,0,Epoch,1,Iteration,5,It. time,0.952757,Elapsed time,35.039671\n",
      "[MO833] Rank,0,Epoch,1,Iteration,6,It. time,3.050581,Elapsed time,38.100549\n",
      "[MO833] Rank,0,Epoch,1,Iteration,7,It. time,1.678781,Elapsed time,39.783194\n",
      "[MO833] Rank,0,Epoch,1,Iteration,8,It. time,1.522361,Elapsed time,41.315423\n",
      "[MO833] Rank,0,Epoch,1,Iteration,9,It. time,2.277458,Elapsed time,43.597799\n",
      "[MO833] Rank,0,Epoch,1,Iteration,10,It. time,2.087863,Elapsed time,45.689766\n",
      "[MO833] Rank,0,Epoch,1,Iteration,11,It. time,0.941250,Elapsed time,46.645743\n",
      "[MO833] Rank,0,Epoch,1,Iteration,12,It. time,0.885670,Elapsed time,47.534824\n",
      "[MO833] Rank,0,Epoch,1,Iteration,13,It. time,1.625385,Elapsed time,49.164707\n",
      "[MO833] Rank,0,Epoch,1,Epoch time,1.627662,Elapsed time,49.166981\n",
      "Epoch 1 - train_one_epoch finished\n",
      "Epoch 1 - after waitall\n",
      "Epoch 2 - train_one_epoch started\n",
      "[MO833] Rank,0,Initialization Time: 56.357440\n",
      "[MO833] Rank,0,Epoch,2,Iteration,0,It. time,0.137946,Elapsed time,56.508937\n",
      "[MO833] Rank,0,Epoch,2,Iteration,1,It. time,1.158250,Elapsed time,57.670274\n",
      "[MO833] Rank,0,Epoch,2,Iteration,2,It. time,2.416730,Elapsed time,60.097003\n",
      "[MO833] Rank,0,Epoch,2,Iteration,3,It. time,0.794191,Elapsed time,60.895810\n",
      "[MO833] Rank,0,Epoch,2,Iteration,4,It. time,2.349086,Elapsed time,63.248256\n",
      "[MO833] Rank,0,Epoch,2,Iteration,5,It. time,0.784925,Elapsed time,64.036491\n",
      "[MO833] Rank,0,Epoch,2,Iteration,6,It. time,1.276028,Elapsed time,65.317694\n",
      "[MO833] Rank,0,Epoch,2,Iteration,7,It. time,2.899731,Elapsed time,68.227886\n",
      "[MO833] Rank,0,Epoch,2,Iteration,8,It. time,1.624512,Elapsed time,69.857131\n",
      "[MO833] Rank,0,Epoch,2,Iteration,9,It. time,3.436479,Elapsed time,73.303999\n",
      "[MO833] Rank,0,Epoch,2,Iteration,10,It. time,2.587154,Elapsed time,75.901919\n",
      "[MO833] Rank,0,Epoch,2,Iteration,11,It. time,2.083334,Elapsed time,77.989507\n",
      "[MO833] Rank,0,Epoch,2,Iteration,12,It. time,0.938155,Elapsed time,78.931188\n",
      "[MO833] Rank,0,Epoch,2,Iteration,13,It. time,1.062183,Elapsed time,79.997215\n",
      "[MO833] Rank,0,Epoch,2,Epoch time,1.064465,Elapsed time,79.999497\n",
      "Epoch 2 - train_one_epoch finished\n",
      "Epoch 2 - after waitall\n",
      "Best model valid Loss=8.4669, valid ppl=4754.7180\n",
      "Best model test Loss=8.4669, test ppl=4754.7180\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import horovod.mxnet as hvd\n",
    "import gluonnlp as nlp\n",
    "import nmt\n",
    "import dataprocessor\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "import utils\n",
    "import random\n",
    "import hyperparameters as hparams\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.cpu(0)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Tranformer')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=64,\n",
    "                    help='training batch size (default: 64)')\n",
    "parser.add_argument('--dtype', type=str, default='float32',\n",
    "                    help='training data type (default: float32)')\n",
    "parser.add_argument('--epochs', type=int, default=10,\n",
    "                    help='number of training epochs (default: 5)')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--no-cuda', type=bool, default=False,\n",
    "                    help='disable training on GPU (default: False)')\n",
    "#args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "#args, _  = parser.parse_known_args()\n",
    "#hparams={\n",
    "#        'src_lang' : 'en',\n",
    "#        'tgt_lang' : 'de'\n",
    "#        }\n",
    "\n",
    "#args.no_cuda = False\n",
    "\n",
    "if not args.no_cuda:\n",
    "    # Disable CUDA if there are no GPUs.\n",
    "    if mx.context.num_gpus() == 0:\n",
    "        args.no_cuda = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(args)\n",
    "\n",
    "# Initialize Horovod\n",
    "hvd.init()\n",
    "\n",
    "# Set context to current process \n",
    "#ctx = mx.cpu(hvd.local_rank()) if args.no_cuda else mx.gpu(hvd.local_rank())\n",
    "\n",
    "num_workers = hvd.size()\n",
    "\n",
    "'''wmt_model_name = 'transformer_en_de_512'\n",
    "wmt_transformer_model, wmt_src_vocab, wmt_tgt_vocab = \\\n",
    "    nlp.model.get_model(wmt_model_name,\n",
    "                              dataset_name='WMT2014',\n",
    "                              pretrained=True,\n",
    "                              ctx=ctx)\n",
    "print(wmt_src_vocab)\n",
    "print(wmt_tgt_vocab)'''\n",
    "\n",
    "# Build model\n",
    "'''encoder, decoder = nmt.transformer.get_transformer_encoder_decoder(units=hparams.num_units,\n",
    "                                                   hidden_size=hparams.hidden_size,\n",
    "                                                   dropout=hparams.dropout,\n",
    "                                                   num_layers=hparams.num_layers,\n",
    "                                                   num_heads=hparams.num_heads,\n",
    "                                                   max_src_length=530,\n",
    "                                                   max_tgt_length=549,\n",
    "                                                   scaled=hparams.scaled)\n",
    "model = nmt.translation.NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder, decoder=decoder,\n",
    "                 share_embed=True, embed_size=hparams.num_units, tie_weights=True,\n",
    "                 embed_initializer=None, prefix='transformer_')\n",
    "model.initialize(init=mx.init.Xavier(magnitude=3.0), ctx=ctx)\n",
    "model.hybridize()'''\n",
    "\n",
    "'''wmt_data_test = nlp.data.WMT2014BPE('newstest2014',\n",
    "                                    src_lang=hparams.src_lang,\n",
    "                                    tgt_lang=hparams.tgt_lang,\n",
    "                                    full=False)\n",
    "print('Source language %s, Target language %s' % (hparams.src_lang, hparams.tgt_lang))\n",
    "wmt_data_test[0]\n",
    "\n",
    "wmt_test_text = nlp.data.WMT2014('newstest2014',\n",
    "                                 src_lang=hparams.src_lang,\n",
    "                                 tgt_lang=hparams.tgt_lang,\n",
    "                                 full=False)\n",
    "wmt_test_text[0]'''\n",
    "\n",
    "demo = True\n",
    "if demo:\n",
    "    dataset = 'TOY'\n",
    "else:\n",
    "    dataset = 'WMT2014BPE'\n",
    "\n",
    "data_train, data_val, data_test, val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab = \\\n",
    "    dataprocessor.load_translation_data(\n",
    "        dataset=dataset,\n",
    "        src_lang=hparams.src_lang,\n",
    "        tgt_lang=hparams.tgt_lang)\n",
    "\n",
    "data_train_lengths = dataprocessor.get_data_lengths(data_train)\n",
    "data_val_lengths = dataprocessor.get_data_lengths(data_val)\n",
    "data_test_lengths = dataprocessor.get_data_lengths(data_test)\n",
    "\n",
    "data_train = data_train.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "data_val = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                           for i, ele in enumerate(data_val)])\n",
    "data_test = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                           for i, ele in enumerate(data_test)])\n",
    "\n",
    "# Create optimizer\n",
    "optimizer_params = {'momentum': args.momentum,\n",
    "                    'learning_rate': args.lr * hvd.size()}\n",
    "opt = mx.optimizer.create('sgd', **optimizer_params)\n",
    "\n",
    "train_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'))\n",
    "test_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack())\n",
    "\n",
    "target_val_lengths = list(map(lambda x: x[-1], data_val_lengths))\n",
    "target_test_lengths = list(map(lambda x: x[-1], data_test_lengths))\n",
    "\n",
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n",
    "                                             batch_size=hparams.batch_size,\n",
    "                                             num_buckets=hparams.num_buckets,\n",
    "                                             ratio=0.0,\n",
    "                                             shuffle=True,\n",
    "                                             use_average_length=True,\n",
    "                                             num_shards=1,\n",
    "                                             bucket_scheme=bucket_scheme)\n",
    "#print('Train Batch Sampler:')\n",
    "#print(train_batch_sampler.stats())\n",
    "\n",
    "val_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_val_lengths,\n",
    "                                       batch_size=hparams.test_batch_size,\n",
    "                                       num_buckets=hparams.num_buckets,\n",
    "                                       ratio=0.0,\n",
    "                                       shuffle=False,\n",
    "                                       use_average_length=True,\n",
    "                                       bucket_scheme=bucket_scheme)\n",
    "#print('Validation Batch Sampler:')\n",
    "#print(val_batch_sampler.stats())\n",
    "\n",
    "test_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_test_lengths,\n",
    "                                        batch_size=hparams.test_batch_size,\n",
    "                                        num_buckets=hparams.num_buckets,\n",
    "                                        ratio=0.0,\n",
    "                                        shuffle=False,\n",
    "                                        use_average_length=True,\n",
    "                                        bucket_scheme=bucket_scheme)\n",
    "#print('Test Batch Sampler:')\n",
    "#print(test_batch_sampler.stats())\n",
    "\n",
    "train_data_loader = nlp.data.ShardedDataLoader(data_train,\n",
    "                                      batch_sampler=train_batch_sampler,\n",
    "                                      batchify_fn=train_batchify_fn,\n",
    "                                      num_workers=8)\n",
    "#print('Length of train_data_loader: %d' % len(train_data_loader))\n",
    "val_data_loader = gluon.data.DataLoader(data_val,\n",
    "                             batch_sampler=val_batch_sampler,\n",
    "                             batchify_fn=test_batchify_fn,\n",
    "                             num_workers=8)\n",
    "#print('Length of val_data_loader: %d' % len(val_data_loader))\n",
    "test_data_loader = gluon.data.DataLoader(data_test,\n",
    "                              batch_sampler=test_batch_sampler,\n",
    "                              batchify_fn=test_batchify_fn,\n",
    "                              num_workers=8)\n",
    "#print('Length of test_data_loader: %d' % len(test_data_loader))\n",
    "\n",
    "# Build model\n",
    "encoder, decoder, one_step_ahead_decoder = nlp.model.transformer.get_transformer_encoder_decoder(units=hparams.num_units,\n",
    "                                                   hidden_size=hparams.hidden_size,\n",
    "                                                   dropout=hparams.dropout,\n",
    "                                                   num_layers=hparams.num_layers,\n",
    "                                                   num_heads=hparams.num_heads,\n",
    "                                                   max_src_length=530,\n",
    "                                                   max_tgt_length=549,\n",
    "                                                   scaled=hparams.scaled)\n",
    "model = nlp.model.translation.NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder, decoder=decoder,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder, share_embed=False, embed_size=hparams.num_units, \n",
    "                                       tie_weights=False, embed_initializer=None, prefix='transformer_')\n",
    "model.initialize(init=mx.init.Xavier(magnitude=3.0), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "#print(model)\n",
    "\n",
    "# Fetch and broadcast parameters\n",
    "params = model.collect_params()\n",
    "if params is not None:\n",
    "    hvd.broadcast_parameters(params, root_rank=0)\n",
    "\n",
    "# Create DistributedTrainer, a subclass of gluon.Trainer\n",
    "trainer = hvd.DistributedTrainer(params,opt)\n",
    "\n",
    "label_smoothing = nlp.loss.LabelSmoothing(epsilon=hparams.epsilon, units=len(tgt_vocab))\n",
    "label_smoothing.hybridize()\n",
    "\n",
    "loss_function = nlp.loss.MaskedSoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "loss_function.hybridize()\n",
    "\n",
    "#test_loss_function = nlp.loss.MaskedSoftmaxCrossEntropyLoss()\n",
    "test_loss_function =  nlp.loss.MaskedSoftmaxCELoss()\n",
    "test_loss_function.hybridize()\n",
    "\n",
    "detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "\n",
    "translator = nmt.translation.BeamSearchTranslator(model=model,\n",
    "                                                  beam_size=hparams.beam_size,\n",
    "                                                  scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha,\n",
    "                                                                                    K=hparams.lp_k),\n",
    "                                                  max_length=200)\n",
    "#print('Use beam_size=%d, alpha=%.2f, K=%d' % (hparams.beam_size, hparams.lp_alpha, hparams.lp_k))\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), hparams.optimizer,\n",
    "                        {'learning_rate': hparams.lr, 'beta2': 0.98, 'epsilon': 1e-9})\n",
    "#print('Use learning_rate=%.2f' % (trainer.learning_rate))\n",
    "\n",
    "# Train model\n",
    "best_valid_loss = float('Inf')\n",
    "step_num = 0\n",
    "#We use warmup steps as introduced in [1].\n",
    "warmup_steps = hparams.warmup_steps\n",
    "grad_interval = hparams.num_accumulated\n",
    "model.collect_params().setattr('grad_req', 'add')\n",
    "#We use Averaging SGD [2] to update the parameters.\n",
    "average_start = (len(train_data_loader) // grad_interval) * \\\n",
    "    (hparams.epochs - hparams.average_start)\n",
    "average_param_dict = {k: mx.nd.array([0]) for k, v in\n",
    "                                      model.collect_params().items()}\n",
    "update_average_param_dict = True\n",
    "model.collect_params().zero_grad()\n",
    "print('Numero de Epochs = %d' % (hparams.epochs))\n",
    "init_time = time.time()\n",
    "for epoch_id in range(hparams.epochs):\n",
    "    print('Epoch %d - train_one_epoch started' % (epoch_id))\n",
    "    utils.train_one_epoch(epoch_id, model, train_data_loader, trainer,\n",
    "                          label_smoothing, loss_function, grad_interval,\n",
    "                          average_param_dict, update_average_param_dict,\n",
    "                          step_num, ctx, hvd.local_rank(), init_time)\n",
    "    print('Epoch %d - train_one_epoch finished' % (epoch_id))\n",
    "    mx.nd.waitall()\n",
    "    print('Epoch %d - after waitall' % (epoch_id))\n",
    "    # We define evaluation function as follows. The `evaluate` function use beam search translator\n",
    "    # to generate outputs for the validation and testing datasets.\n",
    "    '''valid_loss, _ = utils.evaluate(model, val_data_loader,\n",
    "                                   test_loss_function, translator,\n",
    "                                   tgt_vocab, detokenizer, ctx)\n",
    "    print('Epoch %d, valid Loss=%.4f, valid ppl=%.4f'\n",
    "          % (epoch_id, valid_loss, np.exp(valid_loss)))\n",
    "    test_loss, _ = utils.evaluate(model, test_data_loader,\n",
    "                                  test_loss_function, translator,\n",
    "                                  tgt_vocab, detokenizer, ctx)\n",
    "    print('Epoch %d, test Loss=%.4f, test ppl=%.4f'\n",
    "          % (epoch_id, test_loss, np.exp(test_loss)))\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        model.save_parameters('{}.{}'.format(hparams.save_dir, 'valid_best.params'))'''\n",
    "    model.save_parameters('{}.epoch{:d}.params'.format(hparams.save_dir, epoch_id))\n",
    "mx.nd.save('{}.{}'.format(hparams.save_dir, 'average.params'), average_param_dict)\n",
    "\n",
    "if hparams.average_start > 0:\n",
    "    for k, v in model.collect_params().items():\n",
    "        v.set_data(average_param_dict[k])\n",
    "else:\n",
    "    model.load_parameters('{}.{}'.format(hparams.save_dir, 'valid_best.params'), ctx)\n",
    "valid_loss, _ = utils.evaluate(model, val_data_loader,\n",
    "                               test_loss_function, translator,\n",
    "                               tgt_vocab, detokenizer, ctx)\n",
    "print('Best model valid Loss=%.4f, valid ppl=%.4f'\n",
    "      % (valid_loss, np.exp(valid_loss)))\n",
    "test_loss, _ = utils.evaluate(model, test_data_loader,\n",
    "                              test_loss_function, translator,\n",
    "                              tgt_vocab, detokenizer, ctx)\n",
    "print('Best model test Loss=%.4f, test ppl=%.4f'\n",
    "      % (test_loss, np.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
